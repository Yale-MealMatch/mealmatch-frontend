# -*- coding: utf-8 -*-
"""algo2023.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iGwsg6CCeUh44IE3q_fht3nKU3bcEPWQ
"""

!pip3 install supabase

# collect all emails from supabase
from supabase import create_client, Client

url: str = 'https://gfmcdqiayiitfofbxgme.supabase.co'
key: str = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImdmbWNkcWlheWlpdGZvZmJ4Z21lIiwicm9sZSI6ImFub24iLCJpYXQiOjE2NjQ4MjY4MTUsImV4cCI6MTk4MDQwMjgxNX0.BDz1XcqxCywfWGE9Xp19FVu0_865VGsujzsDd0Xru_w'
supabase: Client = create_client(url, key)

import pandas as pd

matches = []
profiles_supabase = supabase.table("profiles").select("*").eq("opt_in_status", "OPTED_IN").execute()
profiles = pd.DataFrame(profiles_supabase.data)
past_matches = supabase.table("matches").select("*").execute()

if len(profiles) % 2 == 1:
    profiles = profiles.sample(frac=1).reset_index(drop=True)
    matches.append((profiles.iloc[0]["email"], 'kai.xu@yale.edu'))
    profiles = profiles.drop(profiles.index[0])
    profiles = profiles.reset_index(drop=True)

group_a = profiles.iloc[0:int(len(profiles)/2)]
group_b = profiles.iloc[int(len(profiles)/2):]

email_to_index_a = {}
for i in range(len(group_a)):
    email_to_index_a[group_a.iloc[i]["email"]] = i
    
email_to_index_b = {}
for i in range(len(group_b)):
    email_to_index_b[group_b.iloc[i]["email"]] = i

!pip3 install nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('stopwords')

import nltk
import string
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

def process_bio(bio):
  # remove punctuations
  bio = bio.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))

  # tokenize words
  tokens = word_tokenize(bio)

  # lemmatize words
  lemmatizer = WordNetLemmatizer()
  lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]

  # stem words
  stemmer = SnowballStemmer("english")
  stemmed_words = [stemmer.stem(word) for word in lemmatized_words]

  # remove stop words
  filtered_words = [word for word in stemmed_words if word not in stopwords.words('english')]

  return set(filtered_words)

import numpy as np

compatibility = np.zeros((len(group_a), len(group_b)))

def get_year_compatibility(i, j, score_mid=0.3):
  output = 0

  if group_b.iloc[j]["year"] in group_a.iloc[i]["year_match"] or group_a.iloc[i]["year"] in group_b.iloc[j]["year_match"]:
    output += score_mid
  if group_b.iloc[j]["year"] in group_a.iloc[i]["year_match"] and group_a.iloc[i]["year"] in group_b.iloc[j]["year_match"]:
    output += (1 - score_mid)

  return output

def get_pronoun_compatibility(i, j, score_mid=0.3):
  output = 0

  pronouns_pref_a = group_a.iloc[i]['pronouns_match']
  pronouns_pref_b = group_b.iloc[j]['pronouns_match']
  pronouns_a = group_a.iloc[i]['pronouns']
  pronouns_b = group_b.iloc[j]['pronouns']

  a_satisfied = (pronouns_pref_a == 'same' and pronouns_b == pronouns_a) or (pronouns_pref_a == 'different' and pronouns_b != pronouns_a) or pronouns_pref_a == 'any'
  b_satisfied = (pronouns_pref_b == 'same' and pronouns_a == pronouns_b) or (pronouns_pref_b == 'different' and pronouns_a != pronouns_b) or pronouns_pref_b == 'any'

  if a_satisfied or b_satisfied:
    output += score_mid
  if a_satisfied and b_satisfied:
    output += (1 - score_mid)

  return output

def get_keyword_compatibility(i, j, score_mid=0.3):
  keywords_a = group_a.iloc[i]['keywords_match']
  bio_a = group_a.iloc[i]['bio']

  keywords_b = group_b.iloc[j]['keywords_match']
  bio_b = group_b.iloc[j]['bio']

  processed_bio_a = process_bio(bio_a)
  processed_bio_b = process_bio(bio_b)

  processed_keywords_a = process_bio(keywords_a)
  processed_keywords_b = process_bio(keywords_b)

  len_processed_a = len(processed_keywords_a)
  len_processed_b = len(processed_keywords_b)
  count_a = 0
  for word in processed_keywords_a:
    if word in bio_b:
      count_a += 1
  
  count_b = 0
  for word in processed_keywords_b:
    if word in bio_a:
      count_b += 1
  count = min(count_a/(len_processed_a + 1), count_b/(len_processed_b + 1))
  return max(count, score_mid)


def solve_compatibility_matrix(weights=[2, 2, 3]):
  for i in range(len(group_a)):
    for j in range(len(group_b)):
      compatibility[i][j] = (weights[0] * get_year_compatibility(i, j) + weights[1] * get_pronoun_compatibility(i, j) + weights[2] * get_keyword_compatibility(i, j)) / sum(weights)

  for i in range(len(past_matches.data)):
    email_a = past_matches.data[i]["from_email"]
    email_b = past_matches.data[i]["to_email"]
    if email_a in email_to_index_a and email_b in email_to_index_b:
      compatibility[email_to_index_a[email_a]][email_to_index_b[email_b]] = -1000000
    elif email_b in email_to_index_a and email_a in email_to_index_b:
      compatibility[email_to_index_a[email_b]][email_to_index_b[email_a]] = -1000000

solve_compatibility_matrix()

import scipy.optimize as sc
import numpy as np
def get_matches(compatibility):
  length = len(compatibility)
  cost = np.zeros((length, length))
  for i in range(length):
    for j in range(length):
      cost[i][j] = 3 - compatibility[i][j]
  row, col = sc.linear_sum_assignment(cost)
  return row, col

def get_pairs(row, col):
  pairs = []
  length = len(row)
  for i in range(length):
    matches.append((group_a.iloc[row[i]]['email'], group_b.iloc[col[i]]['email']))

row, col = get_matches(compatibility)
get_pairs(row, col)
print(matches)